{"cells": [{"cell_type": "markdown", "metadata": {}, "source": "# Credit Limit \u2014 **Classification** (Colab)\nTiers customers into **Low/Med/High** with **Logistic Regression**, **Random Forest**, and **Gradient Boosting**.\n\n**Train/Test split = 80% / 20%**. We report **both Train and Test** metrics."}, {"cell_type": "markdown", "metadata": {}, "source": "## 0) Imports & Setup"}, {"cell_type": "code", "metadata": {}, "source": "import os, joblib, numpy as np, pandas as pd, matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.metrics import accuracy_score, f1_score, confusion_matrix, precision_recall_fscore_support, roc_curve, auc\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\nos.makedirs('figures', exist_ok=True); os.makedirs('artifacts', exist_ok=True)", "outputs": [], "execution_count": null}, {"cell_type": "markdown", "metadata": {}, "source": "## 1) Load data & labels"}, {"cell_type": "code", "metadata": {}, "source": "csv_path = 'Credit_Prediction (3).csv'\ndf = pd.read_csv(csv_path)\ndf.head()", "outputs": [], "execution_count": null}, {"cell_type": "markdown", "metadata": {}, "source": "### Fix: Drop fully-empty columns\nWe drop any column that is entirely missing (e.g., `Unnamed: 19`) to avoid imputation warnings."}, {"cell_type": "code", "metadata": {}, "source": "df = df.dropna(axis=1, how='all')\ndf = df.loc[:, ~df.columns.duplicated()]  # remove duplicate-named cols if any", "outputs": [], "execution_count": null}, {"cell_type": "markdown", "metadata": {}, "source": "Create tertile labels (balanced classes)."}, {"cell_type": "code", "metadata": {}, "source": "y = pd.qcut(df['Credit_Limit'], q=3, labels=['Low','Med','High'])\nX = df.drop(columns=['Credit_Limit'])\nlabels = np.array(['Low','Med','High'])", "outputs": [], "execution_count": null}, {"cell_type": "markdown", "metadata": {}, "source": "## 2) Preprocessing & **80/20** Split"}, {"cell_type": "code", "metadata": {}, "source": "num = X.select_dtypes(include=[np.number]).columns.tolist()\ncat = [c for c in X.columns if c not in num]\npre = ColumnTransformer([\n    ('num', Pipeline([('imputer', SimpleImputer(strategy='median')), ('scaler', StandardScaler())]), num),\n    ('cat', Pipeline([('imputer', SimpleImputer(strategy='most_frequent')), ('onehot', OneHotEncoder(handle_unknown='ignore'))]), cat)\n])\nXtr, Xte, ytr, yte = train_test_split(X, y, test_size=0.20, random_state=42, stratify=y)\nprint('Train size:', Xtr.shape, ' Test size:', Xte.shape)", "outputs": [], "execution_count": null}, {"cell_type": "markdown", "metadata": {}, "source": "## 3) Train & Compare (Train vs Test)"}, {"cell_type": "code", "metadata": {}, "source": "models = {\n  'LogisticRegression': LogisticRegression(max_iter=200),\n  'RandomForestClassifier': RandomForestClassifier(n_estimators=300, max_depth=20, min_samples_split=5, min_samples_leaf=2, max_features='sqrt', random_state=42, n_jobs=-1),\n  'GradientBoostingClassifier': GradientBoostingClassifier(random_state=42)\n}\nrows=[]; best_f1=-1e9; best_name=None; best_pipe=None; best_pred=None; best_prob=None\nfor name, est in models.items():\n    pipe = Pipeline([('preprocess', pre), ('model', est)]).fit(Xtr, ytr)\n    # Test metrics\n    pred_te = pipe.predict(Xte)\n    prob_te = pipe.predict_proba(Xte) if hasattr(pipe.named_steps['model'], 'predict_proba') else None\n    acc_te = accuracy_score(yte, pred_te)\n    f1m_te = f1_score(yte, pred_te, average='macro')\n    # Train metrics\n    pred_tr = pipe.predict(Xtr)\n    acc_tr = accuracy_score(ytr, pred_tr)\n    f1m_tr = f1_score(ytr, pred_tr, average='macro')\n    rows.append({'model':name,\n                'Accuracy_train':acc_tr, 'F1_macro_train':f1m_tr,\n                'Accuracy_test':acc_te,  'F1_macro_test':f1m_te})\n    if f1m_te > best_f1:\n        best_f1, best_name, best_pipe, best_pred, best_prob = f1m_te, name, pipe, pred_te, prob_te\ncls_compare = pd.DataFrame(rows).sort_values('F1_macro_test', ascending=False).round(4)\ndisplay(cls_compare)\ncls_compare.to_csv('artifacts/model_compare_classification_train_test.csv', index=False)", "outputs": [], "execution_count": null}, {"cell_type": "markdown", "metadata": {}, "source": "## 4) Plots & Artifacts (Test)"}, {"cell_type": "code", "metadata": {}, "source": "cm = confusion_matrix(yte, best_pred, labels=labels)\nplt.figure(); plt.imshow(cm, aspect='auto'); plt.title(f'Confusion Matrix \u2014 {best_name} (Test)')\nplt.xlabel('Predicted'); plt.ylabel('Actual'); plt.xticks(range(len(labels)), labels); plt.yticks(range(len(labels)), labels)\nfor i in range(cm.shape[0]):\n    for j in range(cm.shape[1]):\n        plt.text(j, i, str(cm[i, j]), ha='center', va='center')\nplt.tight_layout(); plt.savefig('figures/cls_confusion_matrix_test.png'); plt.show()\nfrom sklearn.preprocessing import label_binarize\nif best_prob is not None:\n    Yb = label_binarize(yte, classes=labels)\n    plt.figure(); aucs=[]\n    for i, k in enumerate(labels):\n        fpr, tpr, _ = roc_curve(Yb[:, i], best_prob[:, i])\n        a = auc(fpr, tpr); aucs.append(a)\n        plt.plot(fpr, tpr, label=f'{k} (AUC={a:.3f})')\n    plt.plot([0,1],[0,1],'--'); plt.xlabel('FPR'); plt.ylabel('TPR'); plt.title(f'ROC (OvR) \u2014 {best_name} (Test)')\n    plt.legend(); plt.tight_layout(); plt.savefig('figures/cls_roc_best_test.png'); plt.show()\nfrom sklearn.metrics import precision_recall_fscore_support\nprec, rec, f1, sup = precision_recall_fscore_support(yte, best_pred, labels=labels, zero_division=0)\nplt.figure(); x=np.arange(len(labels)); w=0.25\nplt.bar(x-w, prec, w, label='Precision'); plt.bar(x, rec, w, label='Recall'); plt.bar(x+w, f1, w, label='F1')\nplt.xticks(x, labels); plt.ylabel('Score'); plt.title(f'Per-Class PRF \u2014 {best_name} (Test)')\nplt.legend(); plt.tight_layout(); plt.savefig('figures/cls_prf_bars_test.png'); plt.show()\njoblib.dump(best_pipe, 'artifacts/best_classification_pipeline.joblib')", "outputs": [], "execution_count": null}, {"cell_type": "markdown", "metadata": {}, "source": "## 5) Final Summary (Train & Test of Winner)"}, {"cell_type": "code", "metadata": {}, "source": "summary = cls_compare.iloc[0:1].copy(); summary.rename(columns={'model':'BestModel'}, inplace=True); display(summary)\nsummary.to_csv('artifacts/final_classification_summary_train_test.csv', index=False)", "outputs": [], "execution_count": null}, {"cell_type": "markdown", "metadata": {}, "source": "## 6) **Quick Test \u2014 Predictions & Accuracy**\nWe preview predictions on a few test rows and print overall **test Accuracy & Macro-F1** for the selected model."}, {"cell_type": "code", "metadata": {}, "source": "import numpy as np, pandas as pd\nfrom sklearn.metrics import accuracy_score, f1_score\nrng = np.random.RandomState(42)\nk = min(10, Xte.shape[0])\nidx = rng.choice(range(Xte.shape[0]), size=k, replace=False)\npred_demo = best_pipe.predict(Xte.iloc[idx])\ntry:\n    prob_demo = best_pipe.predict_proba(Xte.iloc[idx])\n    conf = prob_demo.max(axis=1)\nexcept Exception:\n    prob_demo, conf = None, None\ndemo = pd.DataFrame({'Actual': yte.iloc[idx].values, 'Predicted': pred_demo})\nif conf is not None:\n    demo['Confidence'] = conf\ndisplay(demo)\n\npred_test = best_pipe.predict(Xte)\nacc = accuracy_score(yte, pred_test)\nf1m = f1_score(yte, pred_test, average='macro')\nprint(f'Best Model: {best_name} | Test Accuracy={acc:.4f} | Macro-F1={f1m:.4f}')\n", "outputs": [], "execution_count": null}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"name": "python", "version": "3.11"}}, "nbformat": 4, "nbformat_minor": 5}
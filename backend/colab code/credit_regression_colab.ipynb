{"cells": [{"cell_type": "markdown", "metadata": {}, "source": "# Credit Limit \u2014 **Regression** (Colab)\nCompares **Linear Regression**, **Random Forest Regressor**, and **Gradient Boosting Regressor**.\n\n**Train/Test split = 80% / 20%**. We report **both Train and Test** metrics for each model."}, {"cell_type": "markdown", "metadata": {}, "source": "## 0) Imports & Setup"}, {"cell_type": "code", "metadata": {}, "source": "import os, joblib, numpy as np, pandas as pd, matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\nos.makedirs('figures', exist_ok=True); os.makedirs('artifacts', exist_ok=True)", "outputs": [], "execution_count": null}, {"cell_type": "markdown", "metadata": {}, "source": "## 1) Load data"}, {"cell_type": "code", "metadata": {}, "source": "# from google.colab import files\n# uploaded = files.upload(); csv_path = list(uploaded.keys())[0]\ncsv_path = 'Credit_Prediction (3).csv'\ndf = pd.read_csv(csv_path)\ndf.head()", "outputs": [], "execution_count": null}, {"cell_type": "markdown", "metadata": {}, "source": "### Fix: Drop fully-empty columns\nWe drop any column that is entirely missing (e.g., `Unnamed: 19`) to avoid imputation warnings."}, {"cell_type": "code", "metadata": {}, "source": "df = df.dropna(axis=1, how='all')\ndf = df.loc[:, ~df.columns.duplicated()]  # remove duplicate-named cols if any", "outputs": [], "execution_count": null}, {"cell_type": "markdown", "metadata": {}, "source": "## 2) Quick EDA"}, {"cell_type": "code", "metadata": {}, "source": "display(pd.DataFrame({'rows':[df.shape[0]], 'columns':[df.shape[1]]}))\nnum_cols = df.select_dtypes(include=[np.number]).columns.tolist()\nplt.figure(); df['Credit_Limit'].hist(bins=30); plt.title('Distribution of Credit_Limit'); plt.tight_layout();\nplt.savefig('figures/target_hist.png'); plt.show()\ncorr = df[num_cols].corr(numeric_only=True)['Credit_Limit'].drop('Credit_Limit').sort_values(key=lambda s: s.abs(), ascending=False)\ndisplay(corr.head(12).to_frame('corr_with_target'))", "outputs": [], "execution_count": null}, {"cell_type": "markdown", "metadata": {}, "source": "## 3) Preprocessing & **80/20** Split"}, {"cell_type": "code", "metadata": {}, "source": "X = df.drop(columns=['Credit_Limit']); y = df['Credit_Limit']\nnum = X.select_dtypes(include=[np.number]).columns.tolist()\ncat = [c for c in X.columns if c not in num]\npre = ColumnTransformer([\n    ('num', Pipeline([('imputer', SimpleImputer(strategy='median')), ('scaler', StandardScaler())]), num),\n    ('cat', Pipeline([('imputer', SimpleImputer(strategy='most_frequent')), ('onehot', OneHotEncoder(handle_unknown='ignore'))]), cat)\n])\nXtr, Xte, ytr, yte = train_test_split(X, y, test_size=0.20, random_state=42)  # 80/20\nprint('Train size:', Xtr.shape, ' Test size:', Xte.shape)", "outputs": [], "execution_count": null}, {"cell_type": "markdown", "metadata": {}, "source": "## 4) Train & Compare (Train vs Test)"}, {"cell_type": "code", "metadata": {}, "source": "models = {\n  'LinearRegression': LinearRegression(),\n  'RandomForestRegressor': RandomForestRegressor(n_estimators=300, max_depth=20, min_samples_split=5, min_samples_leaf=2, max_features='sqrt', random_state=42, n_jobs=-1),\n  'GradientBoostingRegressor': GradientBoostingRegressor(random_state=42)\n}\nrows = []; best_r2=-1e9; best_name=None; best_pipe=None; best_pred=None\nfor name, est in models.items():\n    pipe = Pipeline([('preprocess', pre), ('model', est)]).fit(Xtr, ytr)\n    # Test predictions & metrics\n    pred_te = pipe.predict(Xte)\n    r2_te = r2_score(yte, pred_te)\n    mae_te = mean_absolute_error(yte, pred_te)\n    rmse_te = float(np.sqrt(mean_squared_error(yte, pred_te)))\n    # Train predictions & metrics (to gauge over/underfitting)\n    pred_tr = pipe.predict(Xtr)\n    r2_tr = r2_score(ytr, pred_tr)\n    mae_tr = mean_absolute_error(ytr, pred_tr)\n    rmse_tr = float(np.sqrt(mean_squared_error(ytr, pred_tr)))\n    rows.append({'model':name,\n                'R2_train':r2_tr, 'MAE_train':mae_tr, 'RMSE_train':rmse_tr,\n                'R2_test':r2_te,  'MAE_test':mae_te,  'RMSE_test':rmse_te})\n    if r2_te > best_r2:\n        best_r2, best_name, best_pipe, best_pred = r2_te, name, pipe, pred_te\nreg_compare = pd.DataFrame(rows).sort_values('R2_test', ascending=False).round(4)\ndisplay(reg_compare)\nreg_compare.to_csv('artifacts/model_compare_regression_train_test.csv', index=False)", "outputs": [], "execution_count": null}, {"cell_type": "markdown", "metadata": {}, "source": "## 5) Diagnostics & Artifacts (Test)"}, {"cell_type": "code", "metadata": {}, "source": "plt.figure(); plt.scatter(yte, best_pred, s=8)\nlo, hi = float(min(yte.min(), best_pred.min())), float(max(yte.max(), best_pred.max()))\nplt.plot([lo,hi],[lo,hi]); plt.xlabel('Actual'); plt.ylabel('Predicted'); plt.title(f'Pred vs Actual \u2014 {best_name} (Test)'); plt.tight_layout();\nplt.savefig('figures/reg_pred_vs_actual_test.png'); plt.show()\nresid = yte - best_pred\nplt.figure(); plt.hist(resid, bins=40); plt.xlabel('Residual'); plt.ylabel('Count'); plt.title('Residuals \u2014 Best Regression (Test)'); plt.tight_layout();\nplt.savefig('figures/reg_residuals_test.png'); plt.show()\ndec = pd.qcut(pd.Series(best_pred, index=yte.index), 10, labels=False, duplicates='drop')\ncal = pd.DataFrame({'decile':dec, 'actual':yte, 'pred':best_pred}).groupby('decile').mean()\nplt.figure(); plt.plot(cal.index, cal['actual']); plt.plot(cal.index, cal['pred']); plt.xticks(cal.index)\nplt.xlabel('Predicted Decile'); plt.ylabel('Mean Credit_Limit'); plt.title('Calibration by Decile \u2014 Test'); plt.tight_layout();\nplt.savefig('figures/reg_calibration_test.png'); plt.show()\njoblib.dump(best_pipe, 'artifacts/best_regression_pipeline.joblib')", "outputs": [], "execution_count": null}, {"cell_type": "markdown", "metadata": {}, "source": "## 6) Final Summary (Train & Test of Winner)"}, {"cell_type": "code", "metadata": {}, "source": "summary = reg_compare.iloc[0:1].copy(); summary.rename(columns={'model':'BestModel'}, inplace=True); display(summary)\nsummary.to_csv('artifacts/final_regression_summary_train_test.csv', index=False)", "outputs": [], "execution_count": null}, {"cell_type": "markdown", "metadata": {}, "source": "## 7) **Quick Test \u2014 Predictions & Accuracy**\nBelow we:\n1) show a small sample of **Actual vs Predicted** on the test set, and\n2) print the final **test metrics** (R\u00b2, MAE, RMSE) for the selected model."}, {"cell_type": "code", "metadata": {}, "source": "import numpy as np, pandas as pd\nfrom sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n# Sample 10 random test rows for a readable preview\nrng = np.random.RandomState(42)\nk = min(10, Xte.shape[0])\nidx = rng.choice(range(Xte.shape[0]), size=k, replace=False)\npred_demo = best_pipe.predict(Xte.iloc[idx])\ndemo = pd.DataFrame({'Actual': yte.iloc[idx].values, 'Predicted': pred_demo})\ndisplay(demo.round(2))\n\n# Final test metrics for the best model\npred_test = best_pipe.predict(Xte)\nr2 = r2_score(yte, pred_test)\nmae = mean_absolute_error(yte, pred_test)\nrmse = float(np.sqrt(mean_squared_error(yte, pred_test)))\nprint(f'Best Model: {best_name} | Test R\u00b2={r2:.4f} | MAE={mae:.2f} | RMSE={rmse:.2f}')\n", "outputs": [], "execution_count": null}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"name": "python", "version": "3.11"}}, "nbformat": 4, "nbformat_minor": 5}